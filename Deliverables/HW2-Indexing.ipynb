{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0373522",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92d27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f3344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    stemmed = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7971955",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = {}\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    current_doc_no = None\n",
    "    reading_text = False\n",
    "    doc_text = \"\"\n",
    "    \n",
    "    # parse the doc to get doc no and corresponding text\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1', errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if \"<DOCNO>\" in line:\n",
    "            current_doc_no = line.strip().replace('<DOCNO>', '').replace('</DOCNO>', '')\n",
    "        elif \"<TEXT>\" in line:\n",
    "            reading_text = True\n",
    "        elif \"</TEXT>\" in line:\n",
    "            reading_text = False\n",
    "        elif reading_text:\n",
    "            doc_text += line.strip() + ' '\n",
    "        elif \"</DOC>\" in line:\n",
    "            if current_doc_no is not None:\n",
    "                text_map[current_doc_no.strip()] = doc_text.strip()\n",
    "                doc_text = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2055436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing completed\n"
     ]
    }
   ],
   "source": [
    "folder = \"../IR_data/AP_DATA/ap89_collection\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename != 'readme':\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        parse_file(file_path)\n",
    "        \n",
    "print(\"Parsing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7acda6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84678\n"
     ]
    }
   ],
   "source": [
    "#total no of docs\n",
    "print(len(text_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c71878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is Friday, Jan. 20, the 20th day of 1989. There are 345 days left in the year. Today's highlight in history: On Jan. 20, 1981, Iran released the 52 Americans it had held hostage for 444 days, minutes after the presidency passed from Jimmy Carter to Ronald Reagan. On this date: In 1265, England's Parliament, representing districts, cities and boroughs, met for the first time. In 1801, John Marshall was appointed U.S. chief justice. In 1887, the U.S. Senate approved an agreement to lease Pearl Harbor in Hawaii as a naval base. In 1936, Britain's King George V died. He was succeeded by Edward VIII. In 1937, President Franklin D. Roosevelt became the first chief executive to be inaugurated on Jan. 20 instead of March 4, because of the 20th Amendment to the Constitution. In 1942, Nazi officials held the notorious Wannsee conference in Berlin, at which they decided on their ``final solution'' calling for the extermination of Europe's Jews. In 1945, President Franklin D. Roosevelt was sworn into office for an unprecedented fourth term. In 1977, Jimmy Carter was sworn in as the 39th U.S. president, then surprised onlookers as he, his wife Rosalynn and daughter Amy walked from Capitol Hill to the White House, instead of using a limousine. In 1986, the United States observed the first federal holiday in honor of slain civil rights leader Martin Luther King Jr. In 1987, Anglican Church envoy Terry Waite disappeared in Beirut, Lebanon, while attempting to negotiate the release of Western hostages. Ten years ago: Iranian opposition leader Ayatollah Ruhollah Khomeini, living in exile in France, issued a statement to his supporters in Iran, saying, ``I will join you very soon.'' Five years ago: Johnny Weissmuller, who won five Olympic gold medals as a swimmer and went on to movie stardom as Tarzan, died in Acapulco, Mexico, at age 79. One year ago: An Arizona House committee opened hearings on the possible impeachment of Gov. Evan Mecham with testimony that Mecham once tried to thwart an investigation of an alleged death threat made against a former top aide. Today's birthdays: Comedian George Burns is 93. Movie director Federico Fellini is 69. Actor DeForest Kelley is 69. Bandleader Ray Anthony is 67. Actress Patricia Neal is 63. Former astronaut Edwin ``Buzz'' Aldrin is 59. Comedian Arte Johnson is 55. Actress Dorothy Provine is 52. Actor Lorenzo Lamas is 31. Thought for today: ``By the time you're 80 years old you've learned everything. You only have to remember it.'' _ George Burns.\n"
     ]
    }
   ],
   "source": [
    "print(text_map[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02660dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "TOKEN_REGEX = r'\\b[a-zA-Z0-9.]+\\b'\n",
    "\n",
    "def tokenize_text_map(text_map):\n",
    "    tokenized_dict_map = {}\n",
    "\n",
    "    for doc_id, document in text_map.items():\n",
    "        tokens = re.findall(TOKEN_REGEX, document.lower())\n",
    "        tokenized_text = [token for token in tokens]\n",
    "        tokenized_dict_map[doc_id] = tokenized_text\n",
    "\n",
    "    return tokenized_dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158bf4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict_map=tokenize_text_map(text_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07323288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text Map:\n",
      "['today', 'is', 'friday', 'jan', '20', 'the', '20th', 'day', 'of', '1989', 'there', 'are', '345', 'days', 'left', 'in', 'the', 'year', 'today', 's', 'highlight', 'in', 'history', 'on', 'jan', '20', '1981', 'iran', 'released', 'the', '52', 'americans', 'it', 'had', 'held', 'hostage', 'for', '444', 'days', 'minutes', 'after', 'the', 'presidency', 'passed', 'from', 'jimmy', 'carter', 'to', 'ronald', 'reagan', 'on', 'this', 'date', 'in', '1265', 'england', 's', 'parliament', 'representing', 'districts', 'cities', 'and', 'boroughs', 'met', 'for', 'the', 'first', 'time', 'in', '1801', 'john', 'marshall', 'was', 'appointed', 'u.s', 'chief', 'justice', 'in', '1887', 'the', 'u.s', 'senate', 'approved', 'an', 'agreement', 'to', 'lease', 'pearl', 'harbor', 'in', 'hawaii', 'as', 'a', 'naval', 'base', 'in', '1936', 'britain', 's', 'king', 'george', 'v', 'died', 'he', 'was', 'succeeded', 'by', 'edward', 'viii', 'in', '1937', 'president', 'franklin', 'd', 'roosevelt', 'became', 'the', 'first', 'chief', 'executive', 'to', 'be', 'inaugurated', 'on', 'jan', '20', 'instead', 'of', 'march', '4', 'because', 'of', 'the', '20th', 'amendment', 'to', 'the', 'constitution', 'in', '1942', 'nazi', 'officials', 'held', 'the', 'notorious', 'wannsee', 'conference', 'in', 'berlin', 'at', 'which', 'they', 'decided', 'on', 'their', 'final', 'solution', 'calling', 'for', 'the', 'extermination', 'of', 'europe', 's', 'jews', 'in', '1945', 'president', 'franklin', 'd', 'roosevelt', 'was', 'sworn', 'into', 'office', 'for', 'an', 'unprecedented', 'fourth', 'term', 'in', '1977', 'jimmy', 'carter', 'was', 'sworn', 'in', 'as', 'the', '39th', 'u.s', 'president', 'then', 'surprised', 'onlookers', 'as', 'he', 'his', 'wife', 'rosalynn', 'and', 'daughter', 'amy', 'walked', 'from', 'capitol', 'hill', 'to', 'the', 'white', 'house', 'instead', 'of', 'using', 'a', 'limousine', 'in', '1986', 'the', 'united', 'states', 'observed', 'the', 'first', 'federal', 'holiday', 'in', 'honor', 'of', 'slain', 'civil', 'rights', 'leader', 'martin', 'luther', 'king', 'jr', 'in', '1987', 'anglican', 'church', 'envoy', 'terry', 'waite', 'disappeared', 'in', 'beirut', 'lebanon', 'while', 'attempting', 'to', 'negotiate', 'the', 'release', 'of', 'western', 'hostages', 'ten', 'years', 'ago', 'iranian', 'opposition', 'leader', 'ayatollah', 'ruhollah', 'khomeini', 'living', 'in', 'exile', 'in', 'france', 'issued', 'a', 'statement', 'to', 'his', 'supporters', 'in', 'iran', 'saying', 'i', 'will', 'join', 'you', 'very', 'soon', 'five', 'years', 'ago', 'johnny', 'weissmuller', 'who', 'won', 'five', 'olympic', 'gold', 'medals', 'as', 'a', 'swimmer', 'and', 'went', 'on', 'to', 'movie', 'stardom', 'as', 'tarzan', 'died', 'in', 'acapulco', 'mexico', 'at', 'age', '79', 'one', 'year', 'ago', 'an', 'arizona', 'house', 'committee', 'opened', 'hearings', 'on', 'the', 'possible', 'impeachment', 'of', 'gov', 'evan', 'mecham', 'with', 'testimony', 'that', 'mecham', 'once', 'tried', 'to', 'thwart', 'an', 'investigation', 'of', 'an', 'alleged', 'death', 'threat', 'made', 'against', 'a', 'former', 'top', 'aide', 'today', 's', 'birthdays', 'comedian', 'george', 'burns', 'is', '93', 'movie', 'director', 'federico', 'fellini', 'is', '69', 'actor', 'deforest', 'kelley', 'is', '69', 'bandleader', 'ray', 'anthony', 'is', '67', 'actress', 'patricia', 'neal', 'is', '63', 'former', 'astronaut', 'edwin', 'buzz', 'aldrin', 'is', '59', 'comedian', 'arte', 'johnson', 'is', '55', 'actress', 'dorothy', 'provine', 'is', '52', 'actor', 'lorenzo', 'lamas', 'is', '31', 'thought', 'for', 'today', 'by', 'the', 'time', 'you', 're', '80', 'years', 'old', 'you', 've', 'learned', 'everything', 'you', 'only', 'have', 'to', 'remember', 'it', 'george', 'burns']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Text Map:\")\n",
    "print(tokenized_dict_map[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728fc2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "sw_path = \"../config/stoplistnltk.txt\"\n",
    "\n",
    "with open(sw_path) as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2abf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_content(text):\n",
    "    text = ' '.join([word.lower() for word in text.split() if word.lower() not in stopwords])\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87803dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    stemmed = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db17b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict_nonstem_map = {}\n",
    "\n",
    "for doc_id, document in text_map.items():\n",
    "    processed_text = process_content(document)\n",
    "    tokens = processed_text.split()\n",
    "    tokenized_dict_nonstem_map[doc_id] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0185cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized non-stem map:\n",
      "['today', 'friday', 'jan', '20', '20th', '1989', '345', 'days', 'left', 'year', 'todays', 'highlight', 'history', 'jan', '20', '1981', 'iran', 'released', '52', 'americans', 'held', 'hostage', '444', 'days', 'minutes', 'presidency', 'passed', 'jimmy', 'carter', 'ronald', 'reagan', 'date', '1265', 'englands', 'parliament', 'representing', 'districts', 'cities', 'boroughs', 'met', 'time', '1801', 'john', 'marshall', 'appointed', 'us', 'chief', 'justice', '1887', 'us', 'senate', 'approved', 'agreement', 'lease', 'pearl', 'harbor', 'hawaii', 'naval', 'base', '1936', 'britains', 'king', 'george', 'v', 'died', 'succeeded', 'edward', 'viii', '1937', 'president', 'franklin', 'd', 'roosevelt', 'chief', 'executive', 'inaugurated', 'jan', '20', 'march', '4', '20th', 'amendment', 'constitution', '1942', 'nazi', 'officials', 'held', 'notorious', 'wannsee', 'conference', 'berlin', 'decided', 'final', 'solution', 'calling', 'extermination', 'europes', 'jews', '1945', 'president', 'franklin', 'd', 'roosevelt', 'sworn', 'office', 'unprecedented', 'fourth', 'term', '1977', 'jimmy', 'carter', 'sworn', '39th', 'us', 'president', 'surprised', 'onlookers', 'he', 'wife', 'rosalynn', 'daughter', 'amy', 'walked', 'capitol', 'hill', 'white', 'house', 'limousine', '1986', 'united', 'states', 'observed', 'federal', 'holiday', 'honor', 'slain', 'civil', 'rights', 'leader', 'martin', 'luther', 'king', 'jr', '1987', 'anglican', 'church', 'envoy', 'terry', 'waite', 'disappeared', 'beirut', 'lebanon', 'attempting', 'negotiate', 'release', 'western', 'hostages', 'ten', 'years', 'ago', 'iranian', 'opposition', 'leader', 'ayatollah', 'ruhollah', 'khomeini', 'living', 'exile', 'france', 'issued', 'statement', 'supporters', 'iran', 'saying', 'i', 'join', 'soon', 'five', 'years', 'ago', 'johnny', 'weissmuller', 'won', 'five', 'olympic', 'gold', 'medals', 'swimmer', 'went', 'movie', 'stardom', 'tarzan', 'died', 'acapulco', 'mexico', 'age', '79', 'ago', 'arizona', 'house', 'committee', 'opened', 'hearings', 'possible', 'impeachment', 'gov', 'evan', 'mecham', 'testimony', 'mecham', 'tried', 'thwart', 'investigation', 'alleged', 'death', 'threat', 'made', 'former', 'top', 'aide', 'todays', 'birthdays', 'comedian', 'george', 'burns', '93', 'movie', 'director', 'federico', 'fellini', '69', 'actor', 'deforest', 'kelley', '69', 'bandleader', 'ray', 'anthony', '67', 'actress', 'patricia', 'neal', '63', 'former', 'astronaut', 'edwin', 'buzz', 'aldrin', '59', 'comedian', 'arte', 'johnson', '55', 'actress', 'dorothy', 'provine', '52', 'actor', 'lorenzo', 'lamas', '31', 'thought', 'today', 'by', 'time', 'youre', '80', 'years', 'old', 'youve', 'learned', 'everything', 'remember', 'it', 'george', 'burns']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized non-stem map:\")\n",
    "print(tokenized_dict_nonstem_map[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3285f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict_stemmed_map = {}\n",
    "\n",
    "for doc_id, nonstemmed_tokens in tokenized_dict_nonstem_map.items():\n",
    "    stemmed_tokens = [stem_text(token, ps) for token in nonstemmed_tokens]\n",
    "    tokenized_dict_stemmed_map[doc_id] = stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad1934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized stemmed map:\n",
      "['today', 'friday', 'jan', '20', '20th', '1989', '345', 'day', 'left', 'year', 'today', 'highlight', 'histori', 'jan', '20', '1981', 'iran', 'releas', '52', 'american', 'held', 'hostag', '444', 'day', 'minut', 'presid', 'pass', 'jimmi', 'carter', 'ronald', 'reagan', 'date', '1265', 'england', 'parliament', 'repres', 'district', 'citi', 'borough', 'met', 'time', '1801', 'john', 'marshal', 'appoint', 'us', 'chief', 'justic', '1887', 'us', 'senat', 'approv', 'agreement', 'leas', 'pearl', 'harbor', 'hawaii', 'naval', 'base', '1936', 'britain', 'king', 'georg', 'v', 'die', 'succeed', 'edward', 'viii', '1937', 'presid', 'franklin', 'd', 'roosevelt', 'chief', 'execut', 'inaugur', 'jan', '20', 'march', '4', '20th', 'amend', 'constitut', '1942', 'nazi', 'offici', 'held', 'notori', 'wannse', 'confer', 'berlin', 'decid', 'final', 'solut', 'call', 'extermin', 'europ', 'jew', '1945', 'presid', 'franklin', 'd', 'roosevelt', 'sworn', 'offic', 'unpreced', 'fourth', 'term', '1977', 'jimmi', 'carter', 'sworn', '39th', 'us', 'presid', 'surpris', 'onlook', 'he', 'wife', 'rosalynn', 'daughter', 'ami', 'walk', 'capitol', 'hill', 'white', 'hous', 'limousin', '1986', 'unit', 'state', 'observ', 'feder', 'holiday', 'honor', 'slain', 'civil', 'right', 'leader', 'martin', 'luther', 'king', 'jr', '1987', 'anglican', 'church', 'envoy', 'terri', 'wait', 'disappear', 'beirut', 'lebanon', 'attempt', 'negoti', 'releas', 'western', 'hostag', 'ten', 'year', 'ago', 'iranian', 'opposit', 'leader', 'ayatollah', 'ruhollah', 'khomeini', 'live', 'exil', 'franc', 'issu', 'statement', 'support', 'iran', 'say', 'i', 'join', 'soon', 'five', 'year', 'ago', 'johnni', 'weissmul', 'won', 'five', 'olymp', 'gold', 'medal', 'swimmer', 'went', 'movi', 'stardom', 'tarzan', 'die', 'acapulco', 'mexico', 'age', '79', 'ago', 'arizona', 'hous', 'committe', 'open', 'hear', 'possibl', 'impeach', 'gov', 'evan', 'mecham', 'testimoni', 'mecham', 'tri', 'thwart', 'investig', 'alleg', 'death', 'threat', 'made', 'former', 'top', 'aid', 'today', 'birthday', 'comedian', 'georg', 'burn', '93', 'movi', 'director', 'federico', 'fellini', '69', 'actor', 'deforest', 'kelley', '69', 'bandlead', 'ray', 'anthoni', '67', 'actress', 'patricia', 'neal', '63', 'former', 'astronaut', 'edwin', 'buzz', 'aldrin', '59', 'comedian', 'art', 'johnson', '55', 'actress', 'dorothi', 'provin', '52', 'actor', 'lorenzo', 'lama', '31', 'thought', 'today', 'by', 'time', 'your', '80', 'year', 'old', 'youv', 'learn', 'everyth', 'rememb', 'it', 'georg', 'burn']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized stemmed map:\")\n",
    "print(tokenized_dict_stemmed_map[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a781170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict_stemmed_map_data = {}\n",
    "\n",
    "for doc_id, stemmed_tokens in tokenized_dict_stemmed_map.items():\n",
    "    token_positions = {}\n",
    "    for position, token in enumerate(stemmed_tokens, start=1):\n",
    "        if token not in token_positions:\n",
    "            token_positions[token] = []\n",
    "        token_positions[token].append(position)\n",
    "    tokenized_dict_stemmed_map_data[doc_id] = token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1cfb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized stemmed map with term frequency:\n",
      "{'today': [1, 11, 221, 263], 'friday': [2], 'jan': [3, 14, 77], '20': [4, 15, 78], '20th': [5, 81], '1989': [6], '345': [7], 'day': [8, 24], 'left': [9], 'year': [10, 159, 179, 268], 'highlight': [12], 'histori': [13], '1981': [16], 'iran': [17, 173], 'releas': [18, 155], '52': [19, 257], 'american': [20], 'held': [21, 87], 'hostag': [22, 157], '444': [23], 'minut': [25], 'presid': [26, 70, 100, 115], 'pass': [27], 'jimmi': [28, 110], 'carter': [29, 111], 'ronald': [30], 'reagan': [31], 'date': [32], '1265': [33], 'england': [34], 'parliament': [35], 'repres': [36], 'district': [37], 'citi': [38], 'borough': [39], 'met': [40], 'time': [41, 265], '1801': [42], 'john': [43], 'marshal': [44], 'appoint': [45], 'us': [46, 50, 114], 'chief': [47, 74], 'justic': [48], '1887': [49], 'senat': [51], 'approv': [52], 'agreement': [53], 'leas': [54], 'pearl': [55], 'harbor': [56], 'hawaii': [57], 'naval': [58], 'base': [59], '1936': [60], 'britain': [61], 'king': [62, 142], 'georg': [63, 224, 275], 'v': [64], 'die': [65, 193], 'succeed': [66], 'edward': [67], 'viii': [68], '1937': [69], 'franklin': [71, 101], 'd': [72, 102], 'roosevelt': [73, 103], 'execut': [75], 'inaugur': [76], 'march': [79], '4': [80], 'amend': [82], 'constitut': [83], '1942': [84], 'nazi': [85], 'offici': [86], 'notori': [88], 'wannse': [89], 'confer': [90], 'berlin': [91], 'decid': [92], 'final': [93], 'solut': [94], 'call': [95], 'extermin': [96], 'europ': [97], 'jew': [98], '1945': [99], 'sworn': [104, 112], 'offic': [105], 'unpreced': [106], 'fourth': [107], 'term': [108], '1977': [109], '39th': [113], 'surpris': [116], 'onlook': [117], 'he': [118], 'wife': [119], 'rosalynn': [120], 'daughter': [121], 'ami': [122], 'walk': [123], 'capitol': [124], 'hill': [125], 'white': [126], 'hous': [127, 200], 'limousin': [128], '1986': [129], 'unit': [130], 'state': [131], 'observ': [132], 'feder': [133], 'holiday': [134], 'honor': [135], 'slain': [136], 'civil': [137], 'right': [138], 'leader': [139, 163], 'martin': [140], 'luther': [141], 'jr': [143], '1987': [144], 'anglican': [145], 'church': [146], 'envoy': [147], 'terri': [148], 'wait': [149], 'disappear': [150], 'beirut': [151], 'lebanon': [152], 'attempt': [153], 'negoti': [154], 'western': [156], 'ten': [158], 'ago': [160, 180, 198], 'iranian': [161], 'opposit': [162], 'ayatollah': [164], 'ruhollah': [165], 'khomeini': [166], 'live': [167], 'exil': [168], 'franc': [169], 'issu': [170], 'statement': [171], 'support': [172], 'say': [174], 'i': [175], 'join': [176], 'soon': [177], 'five': [178, 184], 'johnni': [181], 'weissmul': [182], 'won': [183], 'olymp': [185], 'gold': [186], 'medal': [187], 'swimmer': [188], 'went': [189], 'movi': [190, 227], 'stardom': [191], 'tarzan': [192], 'acapulco': [194], 'mexico': [195], 'age': [196], '79': [197], 'arizona': [199], 'committe': [201], 'open': [202], 'hear': [203], 'possibl': [204], 'impeach': [205], 'gov': [206], 'evan': [207], 'mecham': [208, 210], 'testimoni': [209], 'tri': [211], 'thwart': [212], 'investig': [213], 'alleg': [214], 'death': [215], 'threat': [216], 'made': [217], 'former': [218, 244], 'top': [219], 'aid': [220], 'birthday': [222], 'comedian': [223, 250], 'burn': [225, 276], '93': [226], 'director': [228], 'federico': [229], 'fellini': [230], '69': [231, 235], 'actor': [232, 258], 'deforest': [233], 'kelley': [234], 'bandlead': [236], 'ray': [237], 'anthoni': [238], '67': [239], 'actress': [240, 254], 'patricia': [241], 'neal': [242], '63': [243], 'astronaut': [245], 'edwin': [246], 'buzz': [247], 'aldrin': [248], '59': [249], 'art': [251], 'johnson': [252], '55': [253], 'dorothi': [255], 'provin': [256], 'lorenzo': [259], 'lama': [260], '31': [261], 'thought': [262], 'by': [264], 'your': [266], '80': [267], 'old': [269], 'youv': [270], 'learn': [271], 'everyth': [272], 'rememb': [273], 'it': [274]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized stemmed map with term frequency:\")\n",
    "print(tokenized_dict_stemmed_map_data[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020f6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dict_nonstem_map_data = {}\n",
    "\n",
    "for doc_id, stemmed_tokens in tokenized_dict_nonstem_map.items():\n",
    "    token_positions = {}\n",
    "    for position, token in enumerate(stemmed_tokens, start=1):\n",
    "        if token not in token_positions:\n",
    "            token_positions[token] = []\n",
    "        token_positions[token].append(position)\n",
    "    tokenized_dict_nonstem_map_data[doc_id] = token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6bb37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized non-stem map with term frequency:\n",
      "{'today': [1, 263], 'friday': [2], 'jan': [3, 14, 77], '20': [4, 15, 78], '20th': [5, 81], '1989': [6], '345': [7], 'days': [8, 24], 'left': [9], 'year': [10], 'todays': [11, 221], 'highlight': [12], 'history': [13], '1981': [16], 'iran': [17, 173], 'released': [18], '52': [19, 257], 'americans': [20], 'held': [21, 87], 'hostage': [22], '444': [23], 'minutes': [25], 'presidency': [26], 'passed': [27], 'jimmy': [28, 110], 'carter': [29, 111], 'ronald': [30], 'reagan': [31], 'date': [32], '1265': [33], 'englands': [34], 'parliament': [35], 'representing': [36], 'districts': [37], 'cities': [38], 'boroughs': [39], 'met': [40], 'time': [41, 265], '1801': [42], 'john': [43], 'marshall': [44], 'appointed': [45], 'us': [46, 50, 114], 'chief': [47, 74], 'justice': [48], '1887': [49], 'senate': [51], 'approved': [52], 'agreement': [53], 'lease': [54], 'pearl': [55], 'harbor': [56], 'hawaii': [57], 'naval': [58], 'base': [59], '1936': [60], 'britains': [61], 'king': [62, 142], 'george': [63, 224, 275], 'v': [64], 'died': [65, 193], 'succeeded': [66], 'edward': [67], 'viii': [68], '1937': [69], 'president': [70, 100, 115], 'franklin': [71, 101], 'd': [72, 102], 'roosevelt': [73, 103], 'executive': [75], 'inaugurated': [76], 'march': [79], '4': [80], 'amendment': [82], 'constitution': [83], '1942': [84], 'nazi': [85], 'officials': [86], 'notorious': [88], 'wannsee': [89], 'conference': [90], 'berlin': [91], 'decided': [92], 'final': [93], 'solution': [94], 'calling': [95], 'extermination': [96], 'europes': [97], 'jews': [98], '1945': [99], 'sworn': [104, 112], 'office': [105], 'unprecedented': [106], 'fourth': [107], 'term': [108], '1977': [109], '39th': [113], 'surprised': [116], 'onlookers': [117], 'he': [118], 'wife': [119], 'rosalynn': [120], 'daughter': [121], 'amy': [122], 'walked': [123], 'capitol': [124], 'hill': [125], 'white': [126], 'house': [127, 200], 'limousine': [128], '1986': [129], 'united': [130], 'states': [131], 'observed': [132], 'federal': [133], 'holiday': [134], 'honor': [135], 'slain': [136], 'civil': [137], 'rights': [138], 'leader': [139, 163], 'martin': [140], 'luther': [141], 'jr': [143], '1987': [144], 'anglican': [145], 'church': [146], 'envoy': [147], 'terry': [148], 'waite': [149], 'disappeared': [150], 'beirut': [151], 'lebanon': [152], 'attempting': [153], 'negotiate': [154], 'release': [155], 'western': [156], 'hostages': [157], 'ten': [158], 'years': [159, 179, 268], 'ago': [160, 180, 198], 'iranian': [161], 'opposition': [162], 'ayatollah': [164], 'ruhollah': [165], 'khomeini': [166], 'living': [167], 'exile': [168], 'france': [169], 'issued': [170], 'statement': [171], 'supporters': [172], 'saying': [174], 'i': [175], 'join': [176], 'soon': [177], 'five': [178, 184], 'johnny': [181], 'weissmuller': [182], 'won': [183], 'olympic': [185], 'gold': [186], 'medals': [187], 'swimmer': [188], 'went': [189], 'movie': [190, 227], 'stardom': [191], 'tarzan': [192], 'acapulco': [194], 'mexico': [195], 'age': [196], '79': [197], 'arizona': [199], 'committee': [201], 'opened': [202], 'hearings': [203], 'possible': [204], 'impeachment': [205], 'gov': [206], 'evan': [207], 'mecham': [208, 210], 'testimony': [209], 'tried': [211], 'thwart': [212], 'investigation': [213], 'alleged': [214], 'death': [215], 'threat': [216], 'made': [217], 'former': [218, 244], 'top': [219], 'aide': [220], 'birthdays': [222], 'comedian': [223, 250], 'burns': [225, 276], '93': [226], 'director': [228], 'federico': [229], 'fellini': [230], '69': [231, 235], 'actor': [232, 258], 'deforest': [233], 'kelley': [234], 'bandleader': [236], 'ray': [237], 'anthony': [238], '67': [239], 'actress': [240, 254], 'patricia': [241], 'neal': [242], '63': [243], 'astronaut': [245], 'edwin': [246], 'buzz': [247], 'aldrin': [248], '59': [249], 'arte': [251], 'johnson': [252], '55': [253], 'dorothy': [255], 'provine': [256], 'lorenzo': [259], 'lamas': [260], '31': [261], 'thought': [262], 'by': [264], 'youre': [266], '80': [267], 'old': [269], 'youve': [270], 'learned': [271], 'everything': [272], 'remember': [273], 'it': [274]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized non-stem map with term frequency:\")\n",
    "print(tokenized_dict_nonstem_map_data[\"AP890109-0262\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c287f52",
   "metadata": {},
   "source": [
    "# Creating index and catalog files in chunks of 1000 for stemmed docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8677214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def write_to_index_file(index_file, index_entries, doc_map):\n",
    "    for term, postings in index_entries.items():\n",
    "        postings_str = ','.join([f\"{doc_map[doc_id]}:[{','.join(map(str, positions))}]\" for doc_id, positions in postings.items()])\n",
    "        index_file.write(f\"{postings_str},\")\n",
    "\n",
    "def write_to_catalog_file(catalog_file, term_offsets):\n",
    "    for term, (offset, size) in sorted(term_offsets.items()): \n",
    "        catalog_file.write(f\"{term} {offset} {size}\\n\")\n",
    "\n",
    "folder_path_index = \"./stemmed_index_files/index/\"\n",
    "folder_path_catalog = \"./stemmed_index_files/catalog/\"\n",
    "os.makedirs(folder_path_index, exist_ok=True)\n",
    "os.makedirs(folder_path_catalog, exist_ok=True)\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "current_index_chunk = 1\n",
    "index_entries = {}\n",
    "index_offset = 0\n",
    "doc_map = {}\n",
    "term_offsets = {}\n",
    "\n",
    "for i, (doc_id, terms) in enumerate(tokenized_dict_stemmed_map_data.items(), start=1):\n",
    "    doc_map[doc_id] = i\n",
    "    for term, positions in terms.items():\n",
    "        if term not in index_entries:\n",
    "            index_entries[term] = {}\n",
    "        index_entries[term][doc_id] = positions\n",
    "\n",
    "    if i % chunk_size == 0:\n",
    "        index_file_path = os.path.join(folder_path_index, f\"index_chunk_{current_index_chunk}.txt\")\n",
    "        catalog_file_path = os.path.join(folder_path_catalog, f\"catalog_chunk_{current_index_chunk}.txt\")\n",
    "        index_offset = 0\n",
    "        with open(index_file_path, \"a\") as index_file, open(catalog_file_path, \"a\") as catalog_file:\n",
    "            for term, postings in index_entries.items():\n",
    "                write_to_index_file(index_file, {term: postings}, doc_map)\n",
    "                term_offsets[term] = (index_offset, index_file.tell() - index_offset)\n",
    "                index_offset = index_file.tell()\n",
    "            \n",
    "            write_to_catalog_file(catalog_file, term_offsets)\n",
    "\n",
    "        index_entries = {}\n",
    "        term_offsets = {}\n",
    "        current_index_chunk += 1\n",
    "\n",
    "if index_entries:\n",
    "    index_file_path = os.path.join(folder_path_index, f\"index_chunk_{current_index_chunk}.txt\")\n",
    "    catalog_file_path = os.path.join(folder_path_catalog, f\"catalog_chunk_{current_index_chunk}.txt\")\n",
    "    index_offset = 0\n",
    "    with open(index_file_path, \"a\") as index_file, open(catalog_file_path, \"a\") as catalog_file:\n",
    "        for term, postings in index_entries.items():\n",
    "            write_to_index_file(index_file, {term: postings}, doc_map)\n",
    "            term_offsets[term] = (index_offset, index_file.tell() - index_offset)\n",
    "            index_offset = index_file.tell() \n",
    "        \n",
    "        write_to_catalog_file(catalog_file, term_offsets)\n",
    "\n",
    "print(\"Indexing completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccd467",
   "metadata": {},
   "source": [
    "# Creating index and catalog files in chunks of 1000 for unstemmed docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2464c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def write_to_index_file(index_file, index_entries, doc_map):\n",
    "    for term, postings in index_entries.items():\n",
    "        postings_str = ','.join([f\"{doc_map[doc_id]}:[{','.join(map(str, positions))}]\" for doc_id, positions in postings.items()])\n",
    "        index_file.write(f\"{postings_str},\")\n",
    "\n",
    "def write_to_catalog_file(catalog_file, term_offsets):\n",
    "    for term, (offset, size) in sorted(term_offsets.items()): \n",
    "        catalog_file.write(f\"{term} {offset} {size}\\n\")\n",
    "\n",
    "\n",
    "folder_path_index = \"./non_stemmed_index_files/index/\"\n",
    "folder_path_catalog = \"./non_stemmed_index_files/catalog/\"\n",
    "os.makedirs(folder_path_index, exist_ok=True)\n",
    "os.makedirs(folder_path_catalog, exist_ok=True)\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "\n",
    "current_index_chunk = 1\n",
    "index_entries = {}\n",
    "index_offset = 0\n",
    "doc_map = {}\n",
    "term_offsets = {}\n",
    "\n",
    "for i, (doc_id, terms) in enumerate(tokenized_dict_nonstem_map_data.items(), start=1):\n",
    "    doc_map[doc_id] = i\n",
    "    for term, positions in terms.items():\n",
    "        if term not in index_entries:\n",
    "            index_entries[term] = {}\n",
    "        index_entries[term][doc_id] = positions\n",
    "\n",
    "    if i % chunk_size == 0:\n",
    "        index_file_path = os.path.join(folder_path_index, f\"index_chunk_{current_index_chunk}.txt\")\n",
    "        catalog_file_path = os.path.join(folder_path_catalog, f\"catalog_chunk_{current_index_chunk}.txt\")\n",
    "        index_offset = 0\n",
    "        with open(index_file_path, \"a\") as index_file, open(catalog_file_path, \"a\") as catalog_file:\n",
    "            for term, postings in index_entries.items():\n",
    "                write_to_index_file(index_file, {term: postings}, doc_map)\n",
    "                term_offsets[term] = (index_offset, index_file.tell() - index_offset)\n",
    "                index_offset = index_file.tell() \n",
    "            \n",
    "            write_to_catalog_file(catalog_file, term_offsets)\n",
    "\n",
    "        index_entries = {}\n",
    "        term_offsets = {}\n",
    "        current_index_chunk += 1\n",
    "\n",
    "if index_entries:\n",
    "    index_file_path = os.path.join(folder_path_index, f\"index_chunk_{current_index_chunk}.txt\")\n",
    "    catalog_file_path = os.path.join(folder_path_catalog, f\"catalog_chunk_{current_index_chunk}.txt\")\n",
    "    index_offset = 0\n",
    "    with open(index_file_path, \"a\") as index_file, open(catalog_file_path, \"a\") as catalog_file:\n",
    "        for term, postings in index_entries.items():\n",
    "            write_to_index_file(index_file, {term: postings}, doc_map)\n",
    "            term_offsets[term] = (index_offset, index_file.tell() - index_offset)\n",
    "            index_offset = index_file.tell()  \n",
    "        \n",
    "        write_to_catalog_file(catalog_file, term_offsets)\n",
    "\n",
    "print(\"Indexing completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87146579",
   "metadata": {},
   "source": [
    "# Creating doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7ba7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_map stored successfully in doc_map.txt file.\n"
     ]
    }
   ],
   "source": [
    "doc_map_file_path = \"./stemmed_index_files/doc_map.txt\"\n",
    "\n",
    "with open(doc_map_file_path, 'w') as doc_map_file:\n",
    "    for doc_id, index in doc_map.items():\n",
    "        doc_map_file.write(f\"{doc_id}: {index}\\n\")\n",
    "\n",
    "print(\"doc_map stored successfully in doc_map.txt file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee9886",
   "metadata": {},
   "source": [
    "# Merging using merge sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bd1cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./stemmed_index_files/index/index_chunk_1.txt ./stemmed_index_files/catalog/catalog_chunk_1.txt\n",
      "./stemmed_index_files/index/index_chunk_2.txt ./stemmed_index_files/catalog/catalog_chunk_2.txt\n",
      "./stemmed_index_files/index/index_chunk_3.txt ./stemmed_index_files/catalog/catalog_chunk_3.txt\n",
      "./stemmed_index_files/index/index_chunk_4.txt ./stemmed_index_files/catalog/catalog_chunk_4.txt\n",
      "./stemmed_index_files/index/index_chunk_5.txt ./stemmed_index_files/catalog/catalog_chunk_5.txt\n",
      "./stemmed_index_files/index/index_chunk_6.txt ./stemmed_index_files/catalog/catalog_chunk_6.txt\n",
      "./stemmed_index_files/index/index_chunk_7.txt ./stemmed_index_files/catalog/catalog_chunk_7.txt\n",
      "./stemmed_index_files/index/index_chunk_8.txt ./stemmed_index_files/catalog/catalog_chunk_8.txt\n",
      "./stemmed_index_files/index/index_chunk_9.txt ./stemmed_index_files/catalog/catalog_chunk_9.txt\n",
      "./stemmed_index_files/index/index_chunk_10.txt ./stemmed_index_files/catalog/catalog_chunk_10.txt\n",
      "./stemmed_index_files/index/index_chunk_11.txt ./stemmed_index_files/catalog/catalog_chunk_11.txt\n",
      "./stemmed_index_files/index/index_chunk_12.txt ./stemmed_index_files/catalog/catalog_chunk_12.txt\n",
      "./stemmed_index_files/index/index_chunk_13.txt ./stemmed_index_files/catalog/catalog_chunk_13.txt\n",
      "./stemmed_index_files/index/index_chunk_14.txt ./stemmed_index_files/catalog/catalog_chunk_14.txt\n",
      "./stemmed_index_files/index/index_chunk_15.txt ./stemmed_index_files/catalog/catalog_chunk_15.txt\n",
      "./stemmed_index_files/index/index_chunk_16.txt ./stemmed_index_files/catalog/catalog_chunk_16.txt\n",
      "./stemmed_index_files/index/index_chunk_17.txt ./stemmed_index_files/catalog/catalog_chunk_17.txt\n",
      "./stemmed_index_files/index/index_chunk_18.txt ./stemmed_index_files/catalog/catalog_chunk_18.txt\n",
      "./stemmed_index_files/index/index_chunk_19.txt ./stemmed_index_files/catalog/catalog_chunk_19.txt\n",
      "./stemmed_index_files/index/index_chunk_20.txt ./stemmed_index_files/catalog/catalog_chunk_20.txt\n",
      "./stemmed_index_files/index/index_chunk_21.txt ./stemmed_index_files/catalog/catalog_chunk_21.txt\n",
      "./stemmed_index_files/index/index_chunk_22.txt ./stemmed_index_files/catalog/catalog_chunk_22.txt\n",
      "./stemmed_index_files/index/index_chunk_23.txt ./stemmed_index_files/catalog/catalog_chunk_23.txt\n",
      "./stemmed_index_files/index/index_chunk_24.txt ./stemmed_index_files/catalog/catalog_chunk_24.txt\n",
      "./stemmed_index_files/index/index_chunk_25.txt ./stemmed_index_files/catalog/catalog_chunk_25.txt\n",
      "./stemmed_index_files/index/index_chunk_26.txt ./stemmed_index_files/catalog/catalog_chunk_26.txt\n",
      "./stemmed_index_files/index/index_chunk_27.txt ./stemmed_index_files/catalog/catalog_chunk_27.txt\n",
      "./stemmed_index_files/index/index_chunk_28.txt ./stemmed_index_files/catalog/catalog_chunk_28.txt\n",
      "./stemmed_index_files/index/index_chunk_29.txt ./stemmed_index_files/catalog/catalog_chunk_29.txt\n",
      "./stemmed_index_files/index/index_chunk_30.txt ./stemmed_index_files/catalog/catalog_chunk_30.txt\n",
      "./stemmed_index_files/index/index_chunk_31.txt ./stemmed_index_files/catalog/catalog_chunk_31.txt\n",
      "./stemmed_index_files/index/index_chunk_32.txt ./stemmed_index_files/catalog/catalog_chunk_32.txt\n",
      "./stemmed_index_files/index/index_chunk_33.txt ./stemmed_index_files/catalog/catalog_chunk_33.txt\n",
      "./stemmed_index_files/index/index_chunk_34.txt ./stemmed_index_files/catalog/catalog_chunk_34.txt\n",
      "./stemmed_index_files/index/index_chunk_35.txt ./stemmed_index_files/catalog/catalog_chunk_35.txt\n",
      "./stemmed_index_files/index/index_chunk_36.txt ./stemmed_index_files/catalog/catalog_chunk_36.txt\n",
      "./stemmed_index_files/index/index_chunk_37.txt ./stemmed_index_files/catalog/catalog_chunk_37.txt\n",
      "./stemmed_index_files/index/index_chunk_38.txt ./stemmed_index_files/catalog/catalog_chunk_38.txt\n",
      "./stemmed_index_files/index/index_chunk_39.txt ./stemmed_index_files/catalog/catalog_chunk_39.txt\n",
      "./stemmed_index_files/index/index_chunk_40.txt ./stemmed_index_files/catalog/catalog_chunk_40.txt\n",
      "./stemmed_index_files/index/index_chunk_41.txt ./stemmed_index_files/catalog/catalog_chunk_41.txt\n",
      "./stemmed_index_files/index/index_chunk_42.txt ./stemmed_index_files/catalog/catalog_chunk_42.txt\n",
      "./stemmed_index_files/index/index_chunk_43.txt ./stemmed_index_files/catalog/catalog_chunk_43.txt\n",
      "./stemmed_index_files/index/index_chunk_44.txt ./stemmed_index_files/catalog/catalog_chunk_44.txt\n",
      "./stemmed_index_files/index/index_chunk_45.txt ./stemmed_index_files/catalog/catalog_chunk_45.txt\n",
      "./stemmed_index_files/index/index_chunk_46.txt ./stemmed_index_files/catalog/catalog_chunk_46.txt\n",
      "./stemmed_index_files/index/index_chunk_47.txt ./stemmed_index_files/catalog/catalog_chunk_47.txt\n",
      "./stemmed_index_files/index/index_chunk_48.txt ./stemmed_index_files/catalog/catalog_chunk_48.txt\n",
      "./stemmed_index_files/index/index_chunk_49.txt ./stemmed_index_files/catalog/catalog_chunk_49.txt\n",
      "./stemmed_index_files/index/index_chunk_50.txt ./stemmed_index_files/catalog/catalog_chunk_50.txt\n",
      "./stemmed_index_files/index/index_chunk_51.txt ./stemmed_index_files/catalog/catalog_chunk_51.txt\n",
      "./stemmed_index_files/index/index_chunk_52.txt ./stemmed_index_files/catalog/catalog_chunk_52.txt\n",
      "./stemmed_index_files/index/index_chunk_53.txt ./stemmed_index_files/catalog/catalog_chunk_53.txt\n",
      "./stemmed_index_files/index/index_chunk_54.txt ./stemmed_index_files/catalog/catalog_chunk_54.txt\n",
      "./stemmed_index_files/index/index_chunk_55.txt ./stemmed_index_files/catalog/catalog_chunk_55.txt\n",
      "./stemmed_index_files/index/index_chunk_56.txt ./stemmed_index_files/catalog/catalog_chunk_56.txt\n",
      "./stemmed_index_files/index/index_chunk_57.txt ./stemmed_index_files/catalog/catalog_chunk_57.txt\n",
      "./stemmed_index_files/index/index_chunk_58.txt ./stemmed_index_files/catalog/catalog_chunk_58.txt\n",
      "./stemmed_index_files/index/index_chunk_59.txt ./stemmed_index_files/catalog/catalog_chunk_59.txt\n",
      "./stemmed_index_files/index/index_chunk_60.txt ./stemmed_index_files/catalog/catalog_chunk_60.txt\n",
      "./stemmed_index_files/index/index_chunk_61.txt ./stemmed_index_files/catalog/catalog_chunk_61.txt\n",
      "./stemmed_index_files/index/index_chunk_62.txt ./stemmed_index_files/catalog/catalog_chunk_62.txt\n",
      "./stemmed_index_files/index/index_chunk_63.txt ./stemmed_index_files/catalog/catalog_chunk_63.txt\n",
      "./stemmed_index_files/index/index_chunk_64.txt ./stemmed_index_files/catalog/catalog_chunk_64.txt\n",
      "./stemmed_index_files/index/index_chunk_65.txt ./stemmed_index_files/catalog/catalog_chunk_65.txt\n",
      "./stemmed_index_files/index/index_chunk_66.txt ./stemmed_index_files/catalog/catalog_chunk_66.txt\n",
      "./stemmed_index_files/index/index_chunk_67.txt ./stemmed_index_files/catalog/catalog_chunk_67.txt\n",
      "./stemmed_index_files/index/index_chunk_68.txt ./stemmed_index_files/catalog/catalog_chunk_68.txt\n",
      "./stemmed_index_files/index/index_chunk_69.txt ./stemmed_index_files/catalog/catalog_chunk_69.txt\n",
      "./stemmed_index_files/index/index_chunk_70.txt ./stemmed_index_files/catalog/catalog_chunk_70.txt\n",
      "./stemmed_index_files/index/index_chunk_71.txt ./stemmed_index_files/catalog/catalog_chunk_71.txt\n",
      "./stemmed_index_files/index/index_chunk_72.txt ./stemmed_index_files/catalog/catalog_chunk_72.txt\n",
      "./stemmed_index_files/index/index_chunk_73.txt ./stemmed_index_files/catalog/catalog_chunk_73.txt\n",
      "./stemmed_index_files/index/index_chunk_74.txt ./stemmed_index_files/catalog/catalog_chunk_74.txt\n",
      "./stemmed_index_files/index/index_chunk_75.txt ./stemmed_index_files/catalog/catalog_chunk_75.txt\n",
      "./stemmed_index_files/index/index_chunk_76.txt ./stemmed_index_files/catalog/catalog_chunk_76.txt\n",
      "./stemmed_index_files/index/index_chunk_77.txt ./stemmed_index_files/catalog/catalog_chunk_77.txt\n",
      "./stemmed_index_files/index/index_chunk_78.txt ./stemmed_index_files/catalog/catalog_chunk_78.txt\n",
      "./stemmed_index_files/index/index_chunk_79.txt ./stemmed_index_files/catalog/catalog_chunk_79.txt\n",
      "./stemmed_index_files/index/index_chunk_80.txt ./stemmed_index_files/catalog/catalog_chunk_80.txt\n",
      "./stemmed_index_files/index/index_chunk_81.txt ./stemmed_index_files/catalog/catalog_chunk_81.txt\n",
      "./stemmed_index_files/index/index_chunk_82.txt ./stemmed_index_files/catalog/catalog_chunk_82.txt\n",
      "./stemmed_index_files/index/index_chunk_83.txt ./stemmed_index_files/catalog/catalog_chunk_83.txt\n",
      "./stemmed_index_files/index/index_chunk_84.txt ./stemmed_index_files/catalog/catalog_chunk_84.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./stemmed_index_files/index/index_chunk_85.txt ./stemmed_index_files/catalog/catalog_chunk_85.txt\n",
      "Merging completed. Merged index and terms files are saved.\n"
     ]
    }
   ],
   "source": [
    "def merge_files(index_file_1, index_file_2, catalog_file_1, catalog_file_2, merged_index_file, merged_terms_file):\n",
    "    def read_content_from_file(file_path, offset, size):\n",
    "        with open(file_path, 'r') as file:\n",
    "            file.seek(offset)\n",
    "            content = file.read(size)\n",
    "        return content\n",
    "    offset = 0\n",
    "    with open(catalog_file_1, 'r') as catalog1, open(catalog_file_2, 'r') as catalog2, \\\n",
    "         open(index_file_1, 'r') as index1, open(index_file_2, 'r') as index2, \\\n",
    "         open(merged_index_file, 'w') as merged_index_f, open(merged_terms_file, 'w') as merged_terms_f:\n",
    "\n",
    "        line1 = catalog1.readline().strip()\n",
    "        line2 = catalog2.readline().strip()\n",
    "        while line1 and line2:\n",
    "            parts1 = line1.split()\n",
    "            parts2 = line2.split()\n",
    "\n",
    "            term1, offset1, size1 = parts1\n",
    "            term2, offset2, size2 = parts2\n",
    "\n",
    "            if term1 < term2:\n",
    "                content = read_content_from_file(index_file_1, int(offset1), int(size1))\n",
    "                merged_index_f.write(content)\n",
    "                merged_terms_f.write(f\"{term1} {offset} {merged_index_f.tell() - int(offset)}\" + '\\n')\n",
    "                offset = merged_index_f.tell()\n",
    "                line1 = catalog1.readline().strip()\n",
    "            elif term1 > term2:\n",
    "                content = read_content_from_file(index_file_2, int(offset2), int(size2))\n",
    "                merged_index_f.write(content)\n",
    "                merged_terms_f.write(f\"{term2} {offset} {merged_index_f.tell() - int(offset)}\" + '\\n')\n",
    "                offset = merged_index_f.tell()\n",
    "                line2 = catalog2.readline().strip()\n",
    "            else:  \n",
    "                content1 = read_content_from_file(index_file_1, int(offset1), int(size1))\n",
    "                content2 = read_content_from_file(index_file_2, int(offset2), int(size2))\n",
    "                merged_index_f.write(content1 + content2)\n",
    "                merged_terms_f.write(f\"{term1} {offset} {merged_index_f.tell() - int(offset)}\" + '\\n')\n",
    "                offset = merged_index_f.tell()\n",
    "                line1 = catalog1.readline().strip()\n",
    "                line2 = catalog2.readline().strip()\n",
    "\n",
    "        while line1:\n",
    "            term1, offset1, size1 = line1.split()\n",
    "            content = read_content_from_file(index_file_1, int(offset1), int(size1))\n",
    "            merged_index_f.write(content)\n",
    "            merged_terms_f.write(f\"{term1} {offset} {merged_index_f.tell() - int(offset)}\" + '\\n')\n",
    "            offset = merged_index_f.tell()\n",
    "            line1 = catalog1.readline().strip()\n",
    "\n",
    "        while line2:\n",
    "            term2, offset2, size2 = line2.split()\n",
    "            content = read_content_from_file(index_file_2, int(offset2), int(size2))\n",
    "            merged_index_f.write(content)\n",
    "            merged_terms_f.write(f\"{term2} {offset} {merged_index_f.tell() - int(offset)}\" + '\\n')\n",
    "            offset = merged_index_f.tell()\n",
    "            line2 = catalog2.readline().strip()\n",
    "\n",
    "folder_path_index = \"./stemmed_index_files/index/\"\n",
    "folder_path_catalog = \"./stemmed_index_files/catalog/\"\n",
    "\n",
    "final_merged_index_file = \"./stemmed_index_files/final_merged_index_file.txt\"\n",
    "final_merged_catalog_file = \"./stemmed_index_files/final_merged_catalog_file.txt\"\n",
    "\n",
    "merged_index_file = \"./stemmed_index_files/merged_index_file.txt\"\n",
    "merged_catalog_file = \"./stemmed_index_files/merged_catalog_file.txt\"\n",
    "\n",
    "open(final_merged_index_file, 'a').close()\n",
    "open(final_merged_catalog_file, 'a').close()\n",
    "open(merged_index_file, 'a').close()\n",
    "open(merged_catalog_file, 'a').close()\n",
    "\n",
    "def sort_files(file_path):\n",
    "    return int(os.path.splitext(os.path.basename(file_path))[0].split('_')[-1])\n",
    "\n",
    "index_files = [os.path.join(folder_path_index, file) for file in sorted(os.listdir(folder_path_index), key=sort_files)]\n",
    "catalog_files = [os.path.join(folder_path_catalog, file) for file in sorted(os.listdir(folder_path_catalog), key=sort_files)]\n",
    "\n",
    "for i in range(len(index_files)):\n",
    "    print(index_files[i], catalog_files[i])\n",
    "    merge_files(merged_index_file, index_files[i],  merged_catalog_file, catalog_files[i], final_merged_index_file, final_merged_catalog_file)\n",
    "\n",
    "    with open(final_merged_index_file, 'r') as final_merged_index_f, open(merged_index_file, 'w') as merged_index_f:\n",
    "        merged_index_f.write(final_merged_index_f.read())\n",
    "    \n",
    "    with open(final_merged_catalog_file, 'r') as final_merged_catalog_f, open(merged_catalog_file, 'w') as merged_catalog_f:\n",
    "        merged_catalog_f.write(final_merged_catalog_f.read())\n",
    "\n",
    "print(\"Merging completed. Merged index and terms files are saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7c254",
   "metadata": {},
   "source": [
    "# Merging unstemmed docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c72b87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./non_stemmed_index_files/index/index_chunk_1.txt ./non_stemmed_index_files/catalog/catalog_chunk_1.txt\n",
      "./non_stemmed_index_files/index/index_chunk_2.txt ./non_stemmed_index_files/catalog/catalog_chunk_2.txt\n",
      "./non_stemmed_index_files/index/index_chunk_3.txt ./non_stemmed_index_files/catalog/catalog_chunk_3.txt\n",
      "./non_stemmed_index_files/index/index_chunk_4.txt ./non_stemmed_index_files/catalog/catalog_chunk_4.txt\n",
      "./non_stemmed_index_files/index/index_chunk_5.txt ./non_stemmed_index_files/catalog/catalog_chunk_5.txt\n",
      "./non_stemmed_index_files/index/index_chunk_6.txt ./non_stemmed_index_files/catalog/catalog_chunk_6.txt\n",
      "./non_stemmed_index_files/index/index_chunk_7.txt ./non_stemmed_index_files/catalog/catalog_chunk_7.txt\n",
      "./non_stemmed_index_files/index/index_chunk_8.txt ./non_stemmed_index_files/catalog/catalog_chunk_8.txt\n",
      "./non_stemmed_index_files/index/index_chunk_9.txt ./non_stemmed_index_files/catalog/catalog_chunk_9.txt\n",
      "./non_stemmed_index_files/index/index_chunk_10.txt ./non_stemmed_index_files/catalog/catalog_chunk_10.txt\n",
      "./non_stemmed_index_files/index/index_chunk_11.txt ./non_stemmed_index_files/catalog/catalog_chunk_11.txt\n",
      "./non_stemmed_index_files/index/index_chunk_12.txt ./non_stemmed_index_files/catalog/catalog_chunk_12.txt\n",
      "./non_stemmed_index_files/index/index_chunk_13.txt ./non_stemmed_index_files/catalog/catalog_chunk_13.txt\n",
      "./non_stemmed_index_files/index/index_chunk_14.txt ./non_stemmed_index_files/catalog/catalog_chunk_14.txt\n",
      "./non_stemmed_index_files/index/index_chunk_15.txt ./non_stemmed_index_files/catalog/catalog_chunk_15.txt\n",
      "./non_stemmed_index_files/index/index_chunk_16.txt ./non_stemmed_index_files/catalog/catalog_chunk_16.txt\n",
      "./non_stemmed_index_files/index/index_chunk_17.txt ./non_stemmed_index_files/catalog/catalog_chunk_17.txt\n",
      "./non_stemmed_index_files/index/index_chunk_18.txt ./non_stemmed_index_files/catalog/catalog_chunk_18.txt\n",
      "./non_stemmed_index_files/index/index_chunk_19.txt ./non_stemmed_index_files/catalog/catalog_chunk_19.txt\n",
      "./non_stemmed_index_files/index/index_chunk_20.txt ./non_stemmed_index_files/catalog/catalog_chunk_20.txt\n",
      "./non_stemmed_index_files/index/index_chunk_21.txt ./non_stemmed_index_files/catalog/catalog_chunk_21.txt\n",
      "./non_stemmed_index_files/index/index_chunk_22.txt ./non_stemmed_index_files/catalog/catalog_chunk_22.txt\n",
      "./non_stemmed_index_files/index/index_chunk_23.txt ./non_stemmed_index_files/catalog/catalog_chunk_23.txt\n",
      "./non_stemmed_index_files/index/index_chunk_24.txt ./non_stemmed_index_files/catalog/catalog_chunk_24.txt\n",
      "./non_stemmed_index_files/index/index_chunk_25.txt ./non_stemmed_index_files/catalog/catalog_chunk_25.txt\n",
      "./non_stemmed_index_files/index/index_chunk_26.txt ./non_stemmed_index_files/catalog/catalog_chunk_26.txt\n",
      "./non_stemmed_index_files/index/index_chunk_27.txt ./non_stemmed_index_files/catalog/catalog_chunk_27.txt\n",
      "./non_stemmed_index_files/index/index_chunk_28.txt ./non_stemmed_index_files/catalog/catalog_chunk_28.txt\n",
      "./non_stemmed_index_files/index/index_chunk_29.txt ./non_stemmed_index_files/catalog/catalog_chunk_29.txt\n",
      "./non_stemmed_index_files/index/index_chunk_30.txt ./non_stemmed_index_files/catalog/catalog_chunk_30.txt\n",
      "./non_stemmed_index_files/index/index_chunk_31.txt ./non_stemmed_index_files/catalog/catalog_chunk_31.txt\n",
      "./non_stemmed_index_files/index/index_chunk_32.txt ./non_stemmed_index_files/catalog/catalog_chunk_32.txt\n",
      "./non_stemmed_index_files/index/index_chunk_33.txt ./non_stemmed_index_files/catalog/catalog_chunk_33.txt\n",
      "./non_stemmed_index_files/index/index_chunk_34.txt ./non_stemmed_index_files/catalog/catalog_chunk_34.txt\n",
      "./non_stemmed_index_files/index/index_chunk_35.txt ./non_stemmed_index_files/catalog/catalog_chunk_35.txt\n",
      "./non_stemmed_index_files/index/index_chunk_36.txt ./non_stemmed_index_files/catalog/catalog_chunk_36.txt\n",
      "./non_stemmed_index_files/index/index_chunk_37.txt ./non_stemmed_index_files/catalog/catalog_chunk_37.txt\n",
      "./non_stemmed_index_files/index/index_chunk_38.txt ./non_stemmed_index_files/catalog/catalog_chunk_38.txt\n",
      "./non_stemmed_index_files/index/index_chunk_39.txt ./non_stemmed_index_files/catalog/catalog_chunk_39.txt\n",
      "./non_stemmed_index_files/index/index_chunk_40.txt ./non_stemmed_index_files/catalog/catalog_chunk_40.txt\n",
      "./non_stemmed_index_files/index/index_chunk_41.txt ./non_stemmed_index_files/catalog/catalog_chunk_41.txt\n",
      "./non_stemmed_index_files/index/index_chunk_42.txt ./non_stemmed_index_files/catalog/catalog_chunk_42.txt\n",
      "./non_stemmed_index_files/index/index_chunk_43.txt ./non_stemmed_index_files/catalog/catalog_chunk_43.txt\n",
      "./non_stemmed_index_files/index/index_chunk_44.txt ./non_stemmed_index_files/catalog/catalog_chunk_44.txt\n",
      "./non_stemmed_index_files/index/index_chunk_45.txt ./non_stemmed_index_files/catalog/catalog_chunk_45.txt\n",
      "./non_stemmed_index_files/index/index_chunk_46.txt ./non_stemmed_index_files/catalog/catalog_chunk_46.txt\n",
      "./non_stemmed_index_files/index/index_chunk_47.txt ./non_stemmed_index_files/catalog/catalog_chunk_47.txt\n",
      "./non_stemmed_index_files/index/index_chunk_48.txt ./non_stemmed_index_files/catalog/catalog_chunk_48.txt\n",
      "./non_stemmed_index_files/index/index_chunk_49.txt ./non_stemmed_index_files/catalog/catalog_chunk_49.txt\n",
      "./non_stemmed_index_files/index/index_chunk_50.txt ./non_stemmed_index_files/catalog/catalog_chunk_50.txt\n",
      "./non_stemmed_index_files/index/index_chunk_51.txt ./non_stemmed_index_files/catalog/catalog_chunk_51.txt\n",
      "./non_stemmed_index_files/index/index_chunk_52.txt ./non_stemmed_index_files/catalog/catalog_chunk_52.txt\n",
      "./non_stemmed_index_files/index/index_chunk_53.txt ./non_stemmed_index_files/catalog/catalog_chunk_53.txt\n",
      "./non_stemmed_index_files/index/index_chunk_54.txt ./non_stemmed_index_files/catalog/catalog_chunk_54.txt\n",
      "./non_stemmed_index_files/index/index_chunk_55.txt ./non_stemmed_index_files/catalog/catalog_chunk_55.txt\n",
      "./non_stemmed_index_files/index/index_chunk_56.txt ./non_stemmed_index_files/catalog/catalog_chunk_56.txt\n",
      "./non_stemmed_index_files/index/index_chunk_57.txt ./non_stemmed_index_files/catalog/catalog_chunk_57.txt\n",
      "./non_stemmed_index_files/index/index_chunk_58.txt ./non_stemmed_index_files/catalog/catalog_chunk_58.txt\n",
      "./non_stemmed_index_files/index/index_chunk_59.txt ./non_stemmed_index_files/catalog/catalog_chunk_59.txt\n",
      "./non_stemmed_index_files/index/index_chunk_60.txt ./non_stemmed_index_files/catalog/catalog_chunk_60.txt\n",
      "./non_stemmed_index_files/index/index_chunk_61.txt ./non_stemmed_index_files/catalog/catalog_chunk_61.txt\n",
      "./non_stemmed_index_files/index/index_chunk_62.txt ./non_stemmed_index_files/catalog/catalog_chunk_62.txt\n",
      "./non_stemmed_index_files/index/index_chunk_63.txt ./non_stemmed_index_files/catalog/catalog_chunk_63.txt\n",
      "./non_stemmed_index_files/index/index_chunk_64.txt ./non_stemmed_index_files/catalog/catalog_chunk_64.txt\n",
      "./non_stemmed_index_files/index/index_chunk_65.txt ./non_stemmed_index_files/catalog/catalog_chunk_65.txt\n",
      "./non_stemmed_index_files/index/index_chunk_66.txt ./non_stemmed_index_files/catalog/catalog_chunk_66.txt\n",
      "./non_stemmed_index_files/index/index_chunk_67.txt ./non_stemmed_index_files/catalog/catalog_chunk_67.txt\n",
      "./non_stemmed_index_files/index/index_chunk_68.txt ./non_stemmed_index_files/catalog/catalog_chunk_68.txt\n",
      "./non_stemmed_index_files/index/index_chunk_69.txt ./non_stemmed_index_files/catalog/catalog_chunk_69.txt\n",
      "./non_stemmed_index_files/index/index_chunk_70.txt ./non_stemmed_index_files/catalog/catalog_chunk_70.txt\n",
      "./non_stemmed_index_files/index/index_chunk_71.txt ./non_stemmed_index_files/catalog/catalog_chunk_71.txt\n",
      "./non_stemmed_index_files/index/index_chunk_72.txt ./non_stemmed_index_files/catalog/catalog_chunk_72.txt\n",
      "./non_stemmed_index_files/index/index_chunk_73.txt ./non_stemmed_index_files/catalog/catalog_chunk_73.txt\n",
      "./non_stemmed_index_files/index/index_chunk_74.txt ./non_stemmed_index_files/catalog/catalog_chunk_74.txt\n",
      "./non_stemmed_index_files/index/index_chunk_75.txt ./non_stemmed_index_files/catalog/catalog_chunk_75.txt\n",
      "./non_stemmed_index_files/index/index_chunk_76.txt ./non_stemmed_index_files/catalog/catalog_chunk_76.txt\n",
      "./non_stemmed_index_files/index/index_chunk_77.txt ./non_stemmed_index_files/catalog/catalog_chunk_77.txt\n",
      "./non_stemmed_index_files/index/index_chunk_78.txt ./non_stemmed_index_files/catalog/catalog_chunk_78.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./non_stemmed_index_files/index/index_chunk_79.txt ./non_stemmed_index_files/catalog/catalog_chunk_79.txt\n",
      "./non_stemmed_index_files/index/index_chunk_80.txt ./non_stemmed_index_files/catalog/catalog_chunk_80.txt\n",
      "./non_stemmed_index_files/index/index_chunk_81.txt ./non_stemmed_index_files/catalog/catalog_chunk_81.txt\n",
      "./non_stemmed_index_files/index/index_chunk_82.txt ./non_stemmed_index_files/catalog/catalog_chunk_82.txt\n",
      "./non_stemmed_index_files/index/index_chunk_83.txt ./non_stemmed_index_files/catalog/catalog_chunk_83.txt\n",
      "./non_stemmed_index_files/index/index_chunk_84.txt ./non_stemmed_index_files/catalog/catalog_chunk_84.txt\n",
      "./non_stemmed_index_files/index/index_chunk_85.txt ./non_stemmed_index_files/catalog/catalog_chunk_85.txt\n",
      "Merging completed. Merged index and terms files are saved.\n"
     ]
    }
   ],
   "source": [
    "folder_path_index = \"./non_stemmed_index_files/index/\"\n",
    "folder_path_catalog = \"./non_stemmed_index_files/catalog/\"\n",
    "\n",
    "final_merged_index_file = \"./non_stemmed_index_files/final_merged_index_file.txt\"\n",
    "final_merged_catalog_file = \"./non_stemmed_index_files/final_merged_catalog_file.txt\"\n",
    "\n",
    "merged_index_file = \"./non_stemmed_index_files/merged_index_file.txt\"\n",
    "merged_catalog_file = \"./non_stemmed_index_files/merged_catalog_file.txt\"\n",
    "\n",
    "open(final_merged_index_file, 'a').close()\n",
    "open(final_merged_catalog_file, 'a').close()\n",
    "open(merged_index_file, 'a').close()\n",
    "open(merged_catalog_file, 'a').close()\n",
    "\n",
    "def sort_files(file_path):\n",
    "    return int(os.path.splitext(os.path.basename(file_path))[0].split('_')[-1])\n",
    "\n",
    "index_files = [os.path.join(folder_path_index, file) for file in sorted(os.listdir(folder_path_index), key=sort_files)]\n",
    "catalog_files = [os.path.join(folder_path_catalog, file) for file in sorted(os.listdir(folder_path_catalog), key=sort_files)]\n",
    "\n",
    "for i in range(len(index_files)):\n",
    "    print(index_files[i], catalog_files[i])\n",
    "    merge_files(merged_index_file, index_files[i],  merged_catalog_file, catalog_files[i], final_merged_index_file, final_merged_catalog_file)\n",
    "\n",
    "    with open(final_merged_index_file, 'r') as final_merged_index_f, open(merged_index_file, 'w') as merged_index_f:\n",
    "        merged_index_f.write(final_merged_index_f.read())\n",
    "    \n",
    "    with open(final_merged_catalog_file, 'r') as final_merged_catalog_f, open(merged_catalog_file, 'w') as merged_catalog_f:\n",
    "        merged_catalog_f.write(final_merged_catalog_f.read())\n",
    "\n",
    "print(\"Merging completed. Merged index and terms files are saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc581905",
   "metadata": {},
   "source": [
    "# Compression into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "941f34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "input_file_path = \"./stemmed_index_files/final_merged_index_file.txt\"\n",
    "output_file_path = \"./stemmed_index_files/final_merged_index_file.gz\"\n",
    "\n",
    "with open(input_file_path, 'rb') as input_file, gzip.open(output_file_path, 'wb') as output_file:\n",
    "    output_file.write(input_file.read())\n",
    "\n",
    "print(\"Compression completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc5110bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths_stemmed = {doc_id: len(tokens) for doc_id, tokens in tokenized_dict_stemmed_map.items()}\n",
    "\n",
    "with open(\"./stemmed_index_files/tokenized_dict_stemmed_length_map.txt\", \"w\") as file:\n",
    "    for doc_id, length in document_lengths_stemmed.items():\n",
    "        file.write(f\"{doc_id} {length}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0b2b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths_non_stemmed = {doc_id: len(tokens) for doc_id, tokens in tokenized_dict_nonstem_map.items()}\n",
    "\n",
    "with open(\"./non_stemmed_index_files/tokenized_dict_non_stemmed_length_map.txt\", \"w\") as file:\n",
    "    for doc_id, length in document_lengths_non_stemmed.items():\n",
    "        file.write(f\"{doc_id} {length}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe9e0a",
   "metadata": {},
   "source": [
    "# Compression into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f67fe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 217712\n",
      "Lines per chunk: 72570\n",
      "Chunks: {'chesebroughpond': ('35891885', '110'), 'motionless': ('111511757', '224'), 'zzzz': ('190606424', '189')}\n"
     ]
    }
   ],
   "source": [
    "catalog_file_path = \"./stemmed_index_files/final_merged_catalog_file.txt\"\n",
    "chunks = {}\n",
    "\n",
    "with open(catalog_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    total_lines = len(lines)\n",
    "\n",
    "    lines_per_chunk = total_lines // 3\n",
    "    remainder_lines = total_lines % 3\n",
    "\n",
    "    start_index = 0\n",
    "    for i in range(3):\n",
    "        chunk_lines = lines_per_chunk\n",
    "        if i < remainder_lines:\n",
    "            chunk_lines += 1\n",
    "        end_index = min(start_index + chunk_lines, total_lines) \n",
    "        if end_index >= len(lines):  \n",
    "            end_index = len(lines)-1\n",
    "            \n",
    "        catalog_line_parts = lines[int(end_index)].split()\n",
    "        word = catalog_line_parts[0]\n",
    "        \n",
    "        offset = catalog_line_parts[1]\n",
    "        size = catalog_line_parts[2]\n",
    "        chunks[word] = (offset, size)\n",
    "        \n",
    "        start_index = end_index\n",
    "\n",
    "print(\"Total number of lines:\", total_lines)\n",
    "print(\"Lines per chunk:\", lines_per_chunk)\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c3fd5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression completed successfully.\n",
      "Compressed chunks: {'chesebroughpond': {'file_path': './stemmed_index_files/compressed_index_files/chunk_1_compressed.gz', 'file_offset': 0}, 'motionless': {'file_path': './stemmed_index_files/compressed_index_files/chunk_2_compressed.gz', 'file_offset': 35891995}, 'zzzz': {'file_path': './stemmed_index_files/compressed_index_files/chunk_3_compressed.gz', 'file_offset': 111511981}}\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"./stemmed_index_files/final_merged_index_file.txt\"\n",
    "output_dir = \"./stemmed_index_files/compressed_index_files/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "compressed_chunks = {}\n",
    "file_offset = 0\n",
    "for i, (chunk_key, (offset, size)) in enumerate(chunks.items(), start=1):\n",
    "    with open(input_file_path, 'rb') as input_file:\n",
    "        input_file.seek(file_offset)\n",
    "        chunk_data = input_file.read(int(offset) + int(size))\n",
    "\n",
    "    compressed_file_path = os.path.join(output_dir, f\"chunk_{i}_compressed.gz\")\n",
    "    with gzip.open(compressed_file_path, 'wb') as output_file:\n",
    "        output_file.write(chunk_data)\n",
    "\n",
    "    compressed_chunks[chunk_key] = {\n",
    "        'file_path': compressed_file_path,\n",
    "        'file_offset': file_offset\n",
    "    }\n",
    "    file_offset = int(offset) + int(size)\n",
    "\n",
    "print(\"Compression completed successfully.\")\n",
    "print(\"Compressed chunks:\", compressed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f3052b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed chunks written to: ./stemmed_index_files/compressed_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_file_path = \"./stemmed_index_files/compressed_chunks.json\"\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(compressed_chunks, output_file)\n",
    "\n",
    "print(\"Compressed chunks written to:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc151069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
